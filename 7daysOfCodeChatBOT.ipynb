{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"TiJXrtBYTFzy"},"outputs":[],"source":["from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n","from langchain.llms import HuggingFacePipeline\n","from langchain.prompts.prompt import PromptTemplate\n","from langchain.chains import ConversationChain\n","from langchain.memory import ConversationBufferWindowMemory\n","\n","# Carregar o modelo e o tokenizador\n","model_name_or_path = \"TheBloke/zephyr-7B-beta-GPTQ\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n","model = AutoModelForCausalLM.from_pretrained(model_name_or_path, device_map=\"auto\")\n","\n","# Criar o pipeline para geração de texto\n","pipe = pipeline(\n","    task=\"text-generation\",\n","    model=model,\n","    tokenizer=tokenizer,\n","    max_new_tokens=512\n",")\n","\n","# Definir o LLM usando o pipeline\n","llm = HuggingFacePipeline(pipeline=pipe)\n","\n","# Criar um template de prompt\n","template = \"\"\"\n","You are a helpful assistant that provides useful information and engages in casual conversation.\n","Respond naturally to user queries and provide useful information.\n","Please, write a single reply only!\n","\n","Current conversation:\n","{history}\n","Question: {input}\n","\"\"\"\n","\n","# Definir o PromptTemplate\n","prompt = PromptTemplate(input_variables=[\"history\", \"input\"], template=template)\n","\n","# Definir a memória para a conversa\n","memory = ConversationBufferWindowMemory(k=3)\n","\n","# Criar a ConversationChain\n","conversation_chain = ConversationChain(\n","    llm=llm,\n","    prompt=prompt,\n","    memory=memory,\n","    verbose=True\n",")\n","\n","# Definir a função get_response\n","def get_response(input_text):\n","    return conversation_chain.predict(input=input_text)\n","\n","# Teste a função\n","input_question = \"How are you?\"\n","response = get_response(input_question)\n","print(response)"]},{"cell_type":"markdown","metadata":{"id":"T_qIt5CK58w-"},"source":[]},{"cell_type":"markdown","metadata":{"id":"NSTE1XogEhAa"},"source":["#### Caso dê um bug na saída, instale este AutoGPTQ Transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rl2w8RctApXp"},"outputs":[],"source":["!pip install auto-gptq transformers"]},{"cell_type":"markdown","metadata":{"id":"X4bPmtbpEplW"},"source":["#### Caso não esteja instalado a biblioteca do AutoGPTQ"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NmietG_CEMQJ"},"outputs":[],"source":["!pip install auto-gptq"]},{"cell_type":"markdown","metadata":{"id":"jaA_yaoCE9Ky"},"source":["#### Provavelmente exigirá esta instalação"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"plga38RoD90B"},"outputs":[],"source":["!pip install langchain_community"]},{"cell_type":"markdown","metadata":{"id":"GkukDXQ0FFSX"},"source":["#### Esta bibilioteca faz parte para o funcionamento do Chatbot (Após instalar reinicie a sessão para entrar em vigor)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"47KoZxvWDF4G"},"outputs":[],"source":["!pip install optimum"]},{"cell_type":"markdown","metadata":{"id":"95B8zC1QFL9h"},"source":["#### Caso dê algum erro na compilação, instale esta biblioteca extra"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wskXJx32CkEQ"},"outputs":[],"source":["!pip install langchain langchain_community"]},{"cell_type":"markdown","metadata":{"id":"gYJF9NeCDunU"},"source":["######INSERIR O TEMPLATE EM PORTUGUÊS"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xkhjVu-WikVQ"},"outputs":[],"source":["template_pt = \"\"\"\n","Você é um assistente útil que fornece informações e participa de conversas do dia a dia.\n","Responda de forma simples, curta e natural, oferecendo informações úteis.\n","Por favor, escreva apenas uma resposta!\n","\n","Conversa atual:\n","{history}\n","Pergunta: {input}\n","\"\"\""]},{"cell_type":"markdown","metadata":{"id":"84SPSD21FdK9"},"source":["### teste bot1 - pergunta"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EtEWeQZEiwrh"},"outputs":[],"source":["input_question = \"Qual é a capital do Brasil?\"\n","response = get_response(input_question)\n","print(response)"]},{"cell_type":"markdown","metadata":{"id":"0rbOOewDFlFb"},"source":["#### teste bot2 - saída"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fAwMiSYRjuYS"},"outputs":[],"source":["print(get_response(\"Olá!\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g21yMIWEjsG8"},"outputs":[],"source":["input_question = \"E qual é a terceira cidade mais populosa?\""]},{"cell_type":"markdown","metadata":{"id":"IoFQt8O1F2_n"},"source":["#### utilize para limpar o histórico das conversas anteriores e criar uma nova conversa ou interação"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zcd1a2h4hNyu"},"outputs":[],"source":["memory.clear()  # limpa o histórico de conversas\n","print(get_response(\"Hoje o dia está como?!\"))"]},{"cell_type":"markdown","metadata":{"id":"Tar99_TuGC9r"},"source":["#### configurando com outro template em português"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pqVs9z9qnhCo"},"outputs":[],"source":["template = \"\"\"\n","Você é um assistente útil que fornece informações úteis e conversa naturalmente em português.\n","Responda às perguntas de maneira clara e direta, apenas em português.\n","Por favor, escreva apenas uma resposta por vez.\n","\n","Conversa atual:\n","{history}\n","Pergunta: {input}\n","\"\"\""]},{"cell_type":"markdown","metadata":{"id":"lKj59ZcWGK9b"},"source":["#### teste bot3 - verificar qual idioma que o chatbot está configurado"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2eP-SIhWnlUi"},"outputs":[],"source":["print(get_response(\"Em que idioma você está respondendo?\"))"]},{"cell_type":"markdown","metadata":{"id":"8X2q3z-1GULU"},"source":["#### configurando o template para responder \"somente\" em português"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vtOXr0Sfn1ce"},"outputs":[],"source":["template = \"\"\"\n","Você é um assistente útil que fornece informações claras e objetivas.\n","Responda **somente** em português.\n","Não use outro idioma.\n","\n","Conversa atual:\n","{history}\n","Pergunta: {input}\n","\"\"\""]},{"cell_type":"markdown","metadata":{"id":"m3ObyWiZGgGk"},"source":["#### teste bot4 - limpando o histórico e verificando em qual idioma está o chatbot"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gL_dp5Cgn3kk"},"outputs":[],"source":["memory.clear()  # Limpa o histórico para garantir que o ajuste tenha efeito\n","print(get_response(\"Em que idioma você está respondendo?\"))"]},{"cell_type":"markdown","metadata":{"id":"B-P2xLfHGtxP"},"source":["#### ajustes para o chatbot datas"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uOP0O_fyo4iC"},"outputs":[],"source":["from datetime import datetime, date\n","\n","# Dicionário com as datas dos feriados e eventos fixos no Brasil\n","feriados = {\n","    \"Ano Novo\": date(datetime.today().year, 1, 1),\n","    \"Carnaval\": date(datetime.today().year, 2, 13),  # Ajuste anual necessário\n","    \"Páscoa\": date(datetime.today().year, 3, 31),  # Ajuste anual necessário\n","    \"Dia do Trabalho\": date(datetime.today().year, 5, 1),\n","    \"Dia dos Namorados\": date(datetime.today().year, 6, 12),\n","    \"Independência do Brasil\": date(datetime.today().year, 9, 7),\n","    \"Dia das Crianças\": date(datetime.today().year, 10, 12),\n","    \"Finados\": date(datetime.today().year, 11, 2),\n","    \"Proclamação da República\": date(datetime.today().year, 11, 15),\n","    \"Natal\": date(datetime.today().year, 12, 25),\n","    \"Dia dos Pais\": None,  # Calculado dinamicamente (segundo domingo de agosto)\n","    \"Dia das Mães\": None   # Calculado dinamicamente (segundo domingo de maio)\n","}\n","\n","# Função para calcular o segundo domingo de um mês\n","def segundo_domingo(mes, ano):\n","    primeiro_dia = date(ano, mes, 1)\n","    return primeiro_dia.replace(day=(8 - primeiro_dia.weekday()))\n","\n","# Definir dinamicamente os dias dos pais e das mães\n","ano_atual = datetime.today().year\n","feriados[\"Dia dos Pais\"] = segundo_domingo(8, ano_atual)\n","feriados[\"Dia das Mães\"] = segundo_domingo(5, ano_atual)\n","\n","# Função para calcular os dias restantes para um evento\n","def dias_ate_evento(evento):\n","    hoje = datetime.today().date()\n","    data_evento = feriados.get(evento)\n","\n","    if data_evento:\n","        dias_faltando = (data_evento - hoje).days\n","        if dias_faltando < 0:\n","            # Se a data já passou neste ano, calcula para o próximo ano\n","            data_evento = data_evento.replace(year=ano_atual + 1)\n","            dias_faltando = (data_evento - hoje).days\n","\n","        return f\"Faltam {dias_faltando} dias para {evento}.\"\n","    else:\n","        return f\"Data de {evento} não encontrada.\"\n","\n","# Testes\n","print(dias_ate_evento(\"Dia dos Pais\"))\n","print(dias_ate_evento(\"Natal\"))\n","print(dias_ate_evento(\"Independência do Brasil\"))\n","print(dias_ate_evento(\"Carnaval\"))"]},{"cell_type":"markdown","metadata":{"id":"ADQMvMA8G5uo"},"source":["#### limpando o histórico e testando as habilidades do chatbot"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BuDWoEbgtHBB"},"outputs":[],"source":["memory.clear()  # Limpa o histórico para garantir que o ajuste tenha efeito\n","print(get_response(\"você é um assistente que precisará responder com informações úteis e conversas do dia a dia. Responda de forma simples, curta e natural com informações úteis.?\"))"]},{"cell_type":"markdown","metadata":{"id":"KyV-f_FA7NOX"},"source":["#### <!> Atenção <!>\n","##### Utilize no lugar da linha de comando \"ngrok config add-authtoken \"inserir o SEU TOKEN AQUI GERADO NO SITE\" e se for disponilizar em seu repositório, remova o seu token e insira um texto indicando onde outro colaborador ou desenvolvedor irá colocar o token dele."]},{"cell_type":"markdown","metadata":{"id":"fA3qVMai_Hsg"},"source":["#### Projeto Completo - 7DaysOfCode"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I7SEcHaTMY80"},"outputs":[],"source":["\n","\n","# Atenção: se você quiser rodar mais de uma vez as células, esta aqui só é necessária uma vez no mesmo ambiente de execução\n","!pip3 install chainlit pyngrok langchain optimum auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/ -q\n","\n","\n","!ngrok config add-authtoken \"INSIRA_O_SEU_TOKEN_GERADO_AQUI\"\n","\n","\n","from pyngrok import ngrok\n","print(ngrok.connect(8000).public_url)\n","\n","\n","%%writefile app.py\n","from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n","from langchain.llms import HuggingFacePipeline\n","from langchain.prompts.prompt import PromptTemplate\n","from langchain.chains import ConversationChain\n","from langchain.memory import ConversationBufferWindowMemory\n","import chainlit as cl\n","\n","model_name_or_path = \"TheBloke/zephyr-7B-beta-GPTQ\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n","model = AutoModelForCausalLM.from_pretrained(model_name_or_path, device_map=\"auto\")\n","\n","pipe = pipeline(\n","    task=\"text-generation\",\n","    model=model,\n","    tokenizer=tokenizer,\n","    max_new_tokens=512\n","    )\n","\n","llm=HuggingFacePipeline(pipeline=pipe)\n","\n","template = \"\"\"\n","\n","You are a helpful assistant that provides information and engages in casual conversation.\n","Respond naturally to user queries and provide useful information.\n","Please, write a single reply only!\n","\n","\n","Current conversation:\n","{history}\n","Question: {input}\n","\n","\n","\"\"\"\n","\n","prompt = PromptTemplate(input_variables=[\"history\", \"input\"], template=template)\n","memory=ConversationBufferWindowMemory(k=3)\n","\n","@cl.on_chat_start\n","async def start():\n","    llm_chain = ConversationChain(prompt=prompt, llm=llm, memory=memory)\n","    cl.user_session.set(\"llm_chain\", llm_chain)\n","\n","@cl.on_message\n","async def main(message: cl.message):\n","    llm_chain = cl.user_session.get(\"llm_chain\")\n","    cb = cl.AsyncLangchainCallbackHandler()\n","    cb.answer_reached = True\n","\n","    res = await cl.make_async(llm_chain)(message.content, callbacks=[cb])\n","\n","    response_text = res['response'].split(\"Answer:\")[-1].strip()\n","\n","    await cl.Message(content=response_text).send()\n","\n","\n","!chainlit run app.py"]},{"cell_type":"markdown","metadata":{"id":"Q3xa2zJc-I3M"},"source":["#### Extra: Em caso de erro da primeira compilação, utilize este abaixo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hq4C_8Ld-XCk"},"outputs":[],"source":["# Instalação das dependências\n","!pip3 install chainlit pyngrok langchain optimum auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/ -q\n","\n","# Configuração do ngrok\n","!ngrok config add-authtoken \"INSIRA_AQUI_O_SEU_TOKEN_GERADO\"\n","\n","# Conectando o ngrok\n","from pyngrok import ngrok\n","print(ngrok.connect(8000).public_url)\n","\n","# Criando o arquivo app.py manualmente\n","app_py_content = \"\"\"\n","from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n","from langchain.llms import HuggingFacePipeline\n","from langchain.prompts.prompt import PromptTemplate\n","from langchain.chains import ConversationChain\n","from langchain.memory import ConversationBufferWindowMemory\n","import chainlit as cl\n","\n","model_name_or_path = \"TheBloke/zephyr-7B-beta-GPTQ\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n","model = AutoModelForCausalLM.from_pretrained(model_name_or_path, device_map=\"auto\")\n","\n","pipe = pipeline(\n","    task=\"text-generation\",\n","    model=model,\n","    tokenizer=tokenizer,\n","    max_new_tokens=512\n","    )\n","\n","llm=HuggingFacePipeline(pipeline=pipe)\n","\n","template = \\\"\"\"\n","You are a helpful assistant that provides information and engages in casual conversation.\n","Respond naturally to user queries and provide useful information.\n","Please, write a single reply only!\n","\n","Current conversation:\n","{history}\n","Question: {input}\n","\\\"\"\"\n","\n","prompt = PromptTemplate(input_variables=[\"history\", \"input\"], template=template)\n","memory=ConversationBufferWindowMemory(k=3)\n","\n","@cl.on_chat_start\n","async def start():\n","    llm_chain = ConversationChain(prompt=prompt, llm=llm, memory=memory)\n","    cl.user_session.set(\"llm_chain\", llm_chain)\n","\n","@cl.on_message\n","async def main(message: cl.message):\n","    llm_chain = cl.user_session.get(\"llm_chain\")\n","    cb = cl.AsyncLangchainCallbackHandler()\n","    cb.answer_reached = True\n","\n","    res = await cl.make_async(llm_chain)(message.content, callbacks=[cb])\n","\n","    response_text = res['response'].split(\"Answer:\")[-1].strip()\n","\n","    await cl.Message(content=response_text).send()\n","\"\"\"\n","\n","# Salvando o conteúdo no arquivo app.py\n","with open(\"app.py\", \"w\") as f:\n","    f.write(app_py_content)\n","\n","# Executando o Chainlit\n","!chainlit run app.py -w"]},{"cell_type":"markdown","metadata":{"id":"BVDDpvJY2185"},"source":["#### Faça o cadastro no site ngrok para gerar o seu Token"]},{"cell_type":"markdown","metadata":{"id":"Td-cn1ou3Fk3"},"source":["#### <!> Importante <!>\n","#### ::::: SEGURANÇA:::::\n","##### Se disponibilizar em seu repositório este chatbot, para a sua segurança: \"remova o seu token e insira o texto no lugar como SEU_TOKEN_AQUI para evitar problemas futuros ou o uso indevido por terceiros.\"\n","\n","Lembre-se: O seu Token é único e exclusivo para o seu uso e ele é a sua identificação digital!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G8MPaO13NBxx"},"outputs":[],"source":["# Atenção: se você quiser rodar mais de uma vez as células, esta aqui só é necessária uma vez no mesmo ambiente de execução\n","!pip3 install chainlit pyngrok langchain optimum auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/ -q\n","\n","\n","!ngrok config add-authtoken \"INSIRA_O_SEU_TOKEN_PESSOAL\" # Replace your_actual_authtoken with your real authtoken\n","\n","\n","from pyngrok import ngrok\n","print(ngrok.connect(8000).public_url)"]},{"cell_type":"markdown","metadata":{"id":"vfyVUxBE8dDF"},"source":["#### Teste do Chatbot final"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sr2s6c-L5oHt"},"outputs":[],"source":["!chainlit run app.py -w"]},{"cell_type":"markdown","metadata":{"id":"OP3HKAEdTfX1"},"source":["#### Se der erro, use este comando abaixo para verificar o Chainlit"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oyViU4wuTqSs"},"outputs":[],"source":["!pip show chainlit"]},{"cell_type":"markdown","metadata":{"id":"1yLn9WD1T03J"},"source":["#### Se estiver ausente, use este coma do para instalá-lo *** Ele irá reiniciar ou reinicie para que entre em vigor *** Logo, teste o chainlit novamente na célula '!chainlit run app.py -w'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t96gwZ2OUA5a"},"outputs":[],"source":["!pip install chainlit"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"authorship_tag":"ABX9TyNTELkEQUeWimu1zT7WoGx3"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}