{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TiJXrtBYTFzy"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.prompts.prompt import PromptTemplate\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "\n",
        "# Carregar o modelo e o tokenizador\n",
        "model_name_or_path = \"TheBloke/zephyr-7B-beta-GPTQ\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name_or_path, device_map=\"auto\")\n",
        "\n",
        "# Criar o pipeline para geração de texto\n",
        "pipe = pipeline(\n",
        "    task=\"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=512\n",
        ")\n",
        "\n",
        "# Definir o LLM usando o pipeline\n",
        "llm = HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "# Criar um template de prompt\n",
        "template = \"\"\"\n",
        "You are a helpful assistant that provides useful information and engages in casual conversation.\n",
        "Respond naturally to user queries and provide useful information.\n",
        "Please, write a single reply only!\n",
        "\n",
        "Current conversation:\n",
        "{history}\n",
        "Question: {input}\n",
        "\"\"\"\n",
        "\n",
        "# Definir o PromptTemplate\n",
        "prompt = PromptTemplate(input_variables=[\"history\", \"input\"], template=template)\n",
        "\n",
        "# Definir a memória para a conversa\n",
        "memory = ConversationBufferWindowMemory(k=3)\n",
        "\n",
        "# Criar a ConversationChain\n",
        "conversation_chain = ConversationChain(\n",
        "    llm=llm,\n",
        "    prompt=prompt,\n",
        "    memory=memory,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Definir a função get_response\n",
        "def get_response(input_text):\n",
        "    return conversation_chain.predict(input=input_text)\n",
        "\n",
        "# Teste a função\n",
        "input_question = \"How are you?\"\n",
        "response = get_response(input_question)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_qIt5CK58w-"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSTE1XogEhAa"
      },
      "source": [
        "#### Caso dê um bug na saída, instale este AutoGPTQ Transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rl2w8RctApXp"
      },
      "outputs": [],
      "source": [
        "!pip install auto-gptq transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4bPmtbpEplW"
      },
      "source": [
        "#### Caso não esteja instalado a biblioteca do AutoGPTQ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NmietG_CEMQJ"
      },
      "outputs": [],
      "source": [
        "!pip install auto-gptq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaA_yaoCE9Ky"
      },
      "source": [
        "#### Provavelmente exigirá esta instalação"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plga38RoD90B"
      },
      "outputs": [],
      "source": [
        "!pip install langchain_community"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkukDXQ0FFSX"
      },
      "source": [
        "#### Esta bibilioteca faz parte para o funcionamento do Chatbot (Após instalar reinicie a sessão para entrar em vigor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47KoZxvWDF4G"
      },
      "outputs": [],
      "source": [
        "!pip install optimum"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95B8zC1QFL9h"
      },
      "source": [
        "#### Caso dê algum erro na compilação, instale esta biblioteca extra"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wskXJx32CkEQ"
      },
      "outputs": [],
      "source": [
        "!pip install langchain langchain_community"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYJF9NeCDunU"
      },
      "source": [
        "######INSERIR O TEMPLATE EM PORTUGUÊS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xkhjVu-WikVQ"
      },
      "outputs": [],
      "source": [
        "template_pt = \"\"\"\n",
        "Você é um assistente útil que fornece informações e participa de conversas do dia a dia.\n",
        "Responda de forma simples, curta e natural, oferecendo informações úteis.\n",
        "Por favor, escreva apenas uma resposta!\n",
        "\n",
        "Conversa atual:\n",
        "{history}\n",
        "Pergunta: {input}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84SPSD21FdK9"
      },
      "source": [
        "### teste bot1 - pergunta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EtEWeQZEiwrh"
      },
      "outputs": [],
      "source": [
        "input_question = \"Qual é a capital do Brasil?\"\n",
        "response = get_response(input_question)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rbOOewDFlFb"
      },
      "source": [
        "#### teste bot2 - saída"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fAwMiSYRjuYS"
      },
      "outputs": [],
      "source": [
        "print(get_response(\"Olá!\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g21yMIWEjsG8"
      },
      "outputs": [],
      "source": [
        "input_question = \"E qual é a terceira cidade mais populosa?\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoFQt8O1F2_n"
      },
      "source": [
        "#### utilize para limpar o histórico das conversas anteriores e criar uma nova conversa ou interação"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zcd1a2h4hNyu"
      },
      "outputs": [],
      "source": [
        "memory.clear()  # limpa o histórico de conversas\n",
        "print(get_response(\"Hoje o dia está como?!\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tar99_TuGC9r"
      },
      "source": [
        "#### configurando com outro template em português"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqVs9z9qnhCo"
      },
      "outputs": [],
      "source": [
        "template = \"\"\"\n",
        "Você é um assistente útil que fornece informações úteis e conversa naturalmente em português.\n",
        "Responda às perguntas de maneira clara e direta, apenas em português.\n",
        "Por favor, escreva apenas uma resposta por vez.\n",
        "\n",
        "Conversa atual:\n",
        "{history}\n",
        "Pergunta: {input}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKj59ZcWGK9b"
      },
      "source": [
        "#### teste bot3 - verificar qual idioma que o chatbot está configurado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2eP-SIhWnlUi"
      },
      "outputs": [],
      "source": [
        "print(get_response(\"Em que idioma você está respondendo?\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8X2q3z-1GULU"
      },
      "source": [
        "#### configurando o template para responder \"somente\" em português"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vtOXr0Sfn1ce"
      },
      "outputs": [],
      "source": [
        "template = \"\"\"\n",
        "Você é um assistente útil que fornece informações claras e objetivas.\n",
        "Responda **somente** em português.\n",
        "Não use outro idioma.\n",
        "\n",
        "Conversa atual:\n",
        "{history}\n",
        "Pergunta: {input}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3ObyWiZGgGk"
      },
      "source": [
        "#### teste bot4 - limpando o histórico e verificando em qual idioma está o chatbot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gL_dp5Cgn3kk"
      },
      "outputs": [],
      "source": [
        "memory.clear()  # Limpa o histórico para garantir que o ajuste tenha efeito\n",
        "print(get_response(\"Em que idioma você está respondendo?\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-P2xLfHGtxP"
      },
      "source": [
        "#### ajustes para o chatbot datas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uOP0O_fyo4iC"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime, date\n",
        "\n",
        "# Dicionário com as datas dos feriados e eventos fixos no Brasil\n",
        "feriados = {\n",
        "    \"Ano Novo\": date(datetime.today().year, 1, 1),\n",
        "    \"Carnaval\": date(datetime.today().year, 2, 13),  # Ajuste anual necessário\n",
        "    \"Páscoa\": date(datetime.today().year, 3, 31),  # Ajuste anual necessário\n",
        "    \"Dia do Trabalho\": date(datetime.today().year, 5, 1),\n",
        "    \"Dia dos Namorados\": date(datetime.today().year, 6, 12),\n",
        "    \"Independência do Brasil\": date(datetime.today().year, 9, 7),\n",
        "    \"Dia das Crianças\": date(datetime.today().year, 10, 12),\n",
        "    \"Finados\": date(datetime.today().year, 11, 2),\n",
        "    \"Proclamação da República\": date(datetime.today().year, 11, 15),\n",
        "    \"Natal\": date(datetime.today().year, 12, 25),\n",
        "    \"Dia dos Pais\": None,  # Calculado dinamicamente (segundo domingo de agosto)\n",
        "    \"Dia das Mães\": None   # Calculado dinamicamente (segundo domingo de maio)\n",
        "}\n",
        "\n",
        "# Função para calcular o segundo domingo de um mês\n",
        "def segundo_domingo(mes, ano):\n",
        "    primeiro_dia = date(ano, mes, 1)\n",
        "    return primeiro_dia.replace(day=(8 - primeiro_dia.weekday()))\n",
        "\n",
        "# Definir dinamicamente os dias dos pais e das mães\n",
        "ano_atual = datetime.today().year\n",
        "feriados[\"Dia dos Pais\"] = segundo_domingo(8, ano_atual)\n",
        "feriados[\"Dia das Mães\"] = segundo_domingo(5, ano_atual)\n",
        "\n",
        "# Função para calcular os dias restantes para um evento\n",
        "def dias_ate_evento(evento):\n",
        "    hoje = datetime.today().date()\n",
        "    data_evento = feriados.get(evento)\n",
        "\n",
        "    if data_evento:\n",
        "        dias_faltando = (data_evento - hoje).days\n",
        "        if dias_faltando < 0:\n",
        "            # Se a data já passou neste ano, calcula para o próximo ano\n",
        "            data_evento = data_evento.replace(year=ano_atual + 1)\n",
        "            dias_faltando = (data_evento - hoje).days\n",
        "\n",
        "        return f\"Faltam {dias_faltando} dias para {evento}.\"\n",
        "    else:\n",
        "        return f\"Data de {evento} não encontrada.\"\n",
        "\n",
        "# Testes\n",
        "print(dias_ate_evento(\"Dia dos Pais\"))\n",
        "print(dias_ate_evento(\"Natal\"))\n",
        "print(dias_ate_evento(\"Independência do Brasil\"))\n",
        "print(dias_ate_evento(\"Carnaval\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADQMvMA8G5uo"
      },
      "source": [
        "#### limpando o histórico e testando as habilidades do chatbot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BuDWoEbgtHBB"
      },
      "outputs": [],
      "source": [
        "memory.clear()  # Limpa o histórico para garantir que o ajuste tenha efeito\n",
        "print(get_response(\"você é um assistente que precisará responder com informações úteis e conversas do dia a dia. Responda de forma simples, curta e natural com informações úteis.?\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyV-f_FA7NOX"
      },
      "source": [
        "#### <!> Atenção <!>\n",
        "##### Utilize no lugar da linha de comando \"ngrok config add-authtoken \"inserir o SEU TOKEN AQUI GERADO NO SITE\" e se for disponilizar em seu repositório, remova o seu token e insira um texto indicando onde outro colaborador ou desenvolvedor irá colocar o token dele."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fA3qVMai_Hsg"
      },
      "source": [
        "#### Projeto Completo - 7DaysOfCode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7SEcHaTMY80"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Atenção: se você quiser rodar mais de uma vez as células, esta aqui só é necessária uma vez no mesmo ambiente de execução\n",
        "!pip3 install chainlit pyngrok langchain optimum auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/ -q\n",
        "\n",
        "\n",
        "!ngrok config add-authtoken \"2tBX42Q7po6pD4usJ9lkFQ8CuZK_7oXGJvRGWYPeaYKa4s7f7\"\n",
        "\n",
        "\n",
        "from pyngrok import ngrok\n",
        "print(ngrok.connect(8000).public_url)\n",
        "\n",
        "\n",
        "%%writefile app.py\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.prompts.prompt import PromptTemplate\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "import chainlit as cl\n",
        "\n",
        "model_name_or_path = \"TheBloke/zephyr-7B-beta-GPTQ\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name_or_path, device_map=\"auto\")\n",
        "\n",
        "pipe = pipeline(\n",
        "    task=\"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=512\n",
        "    )\n",
        "\n",
        "llm=HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "template = \"\"\"\n",
        "\n",
        "You are a helpful assistant that provides information and engages in casual conversation.\n",
        "Respond naturally to user queries and provide useful information.\n",
        "Please, write a single reply only!\n",
        "\n",
        "\n",
        "Current conversation:\n",
        "{history}\n",
        "Question: {input}\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(input_variables=[\"history\", \"input\"], template=template)\n",
        "memory=ConversationBufferWindowMemory(k=3)\n",
        "\n",
        "@cl.on_chat_start\n",
        "async def start():\n",
        "    llm_chain = ConversationChain(prompt=prompt, llm=llm, memory=memory)\n",
        "    cl.user_session.set(\"llm_chain\", llm_chain)\n",
        "\n",
        "@cl.on_message\n",
        "async def main(message: cl.message):\n",
        "    llm_chain = cl.user_session.get(\"llm_chain\")\n",
        "    cb = cl.AsyncLangchainCallbackHandler()\n",
        "    cb.answer_reached = True\n",
        "\n",
        "    res = await cl.make_async(llm_chain)(message.content, callbacks=[cb])\n",
        "\n",
        "    response_text = res['response'].split(\"Answer:\")[-1].strip()\n",
        "\n",
        "    await cl.Message(content=response_text).send()\n",
        "\n",
        "\n",
        "!chainlit run app.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3xa2zJc-I3M"
      },
      "source": [
        "#### Extra: Em caso de erro da primeira compilação, utilize este abaixo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hq4C_8Ld-XCk",
        "outputId": "0597c6c7-fa18-47c4-b13e-739f2268f758"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n",
            "https://c573-34-141-163-103.ngrok-free.app\n",
            "2025-02-20 21:14:45.775353: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1740086085.814810    3511 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1740086085.834181    3511 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2025-02-20T21:14:48+0000 lvl=warn msg=\"failed to open private leg\" id=fa1ccb5e4ce8 privaddr=localhost:8000 err=\"dial tcp 127.0.0.1:8000: connect: connection refused\"\n",
            "WARNING:pyngrok.process.ngrok:t=2025-02-20T21:14:52+0000 lvl=warn msg=\"failed to open private leg\" id=ba968b5de28e privaddr=localhost:8000 err=\"dial tcp 127.0.0.1:8000: connect: connection refused\"\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain/llms/__init__.py:549: LangChainDeprecationWarning: Importing LLMs from langchain is deprecated. Importing from langchain will no longer be supported as of langchain==0.2.0. Please import from langchain-community instead:\n",
            "\n",
            "`from langchain_community.llms import HuggingFacePipeline`.\n",
            "\n",
            "To install langchain-community run `pip install -U langchain-community`.\n",
            "  warnings.warn(\n",
            "tokenizer_config.json: 100% 1.43k/1.43k [00:00<00:00, 6.12MB/s]\n",
            "tokenizer.model: 100% 493k/493k [00:00<00:00, 13.1MB/s]\n",
            "tokenizer.json: 100% 1.80M/1.80M [00:00<00:00, 5.26MB/s]\n",
            "special_tokens_map.json: 100% 168/168 [00:00<00:00, 912kB/s]\n",
            "config.json: 100% 1.31k/1.31k [00:00<00:00, 8.11MB/s]\n",
            "Using `disable_exllama` is deprecated and will be removed in version 4.37. Use `use_exllama` instead and specify the version with `exllama_config`.The value of `use_exllama` will be overwritten by `disable_exllama` passed in `GPTQConfig` or stored in your config file.\n",
            "2025-02-20 21:14:56 - PyTorch version 2.5.1+cu124 available.\n",
            "2025-02-20 21:14:56 - Polars version 1.9.0 available.\n",
            "2025-02-20 21:14:56 - Duckdb version 1.1.3 available.\n",
            "2025-02-20 21:14:56 - TensorFlow version 2.18.0 available.\n",
            "2025-02-20 21:14:56 - JAX version 0.4.33 available.\n",
            "/usr/local/lib/python3.11/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:410: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  @custom_fwd\n",
            "/usr/local/lib/python3.11/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:418: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "  @custom_bwd\n",
            "/usr/local/lib/python3.11/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  @custom_fwd(cast_inputs=torch.float16)\n",
            "2025-02-20 21:14:57 - CUDA extension not installed.\n",
            "2025-02-20 21:14:57 - CUDA extension not installed.\n",
            "model.safetensors: 100% 4.16G/4.16G [01:38<00:00, 42.2MB/s]\n",
            "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n",
            "2025-02-20 21:16:37 - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
            "Some weights of the model checkpoint at TheBloke/zephyr-7B-beta-GPTQ were not used when initializing MistralForCausalLM: {'model.layers.7.mlp.gate_proj.bias', 'model.layers.2.self_attn.o_proj.bias', 'model.layers.25.mlp.up_proj.bias', 'model.layers.9.self_attn.q_proj.bias', 'model.layers.15.self_attn.o_proj.bias', 'model.layers.26.mlp.up_proj.bias', 'model.layers.5.mlp.gate_proj.bias', 'model.layers.11.self_attn.o_proj.bias', 'model.layers.13.self_attn.k_proj.bias', 'model.layers.11.self_attn.v_proj.bias', 'model.layers.16.self_attn.q_proj.bias', 'model.layers.6.mlp.down_proj.bias', 'model.layers.27.self_attn.q_proj.bias', 'model.layers.13.mlp.down_proj.bias', 'model.layers.16.mlp.gate_proj.bias', 'model.layers.31.self_attn.k_proj.bias', 'model.layers.15.mlp.gate_proj.bias', 'model.layers.22.mlp.up_proj.bias', 'model.layers.13.self_attn.v_proj.bias', 'model.layers.13.mlp.up_proj.bias', 'model.layers.12.self_attn.k_proj.bias', 'model.layers.19.mlp.up_proj.bias', 'model.layers.28.self_attn.o_proj.bias', 'model.layers.20.self_attn.q_proj.bias', 'model.layers.1.mlp.gate_proj.bias', 'model.layers.17.mlp.gate_proj.bias', 'model.layers.14.self_attn.v_proj.bias', 'model.layers.27.self_attn.o_proj.bias', 'model.layers.15.self_attn.q_proj.bias', 'model.layers.19.self_attn.v_proj.bias', 'model.layers.18.mlp.down_proj.bias', 'model.layers.24.mlp.gate_proj.bias', 'model.layers.6.self_attn.k_proj.bias', 'model.layers.20.mlp.down_proj.bias', 'model.layers.1.mlp.down_proj.bias', 'model.layers.17.self_attn.v_proj.bias', 'model.layers.25.self_attn.q_proj.bias', 'model.layers.31.mlp.gate_proj.bias', 'model.layers.3.self_attn.o_proj.bias', 'model.layers.29.mlp.up_proj.bias', 'model.layers.31.mlp.up_proj.bias', 'model.layers.19.self_attn.o_proj.bias', 'model.layers.28.mlp.gate_proj.bias', 'model.layers.30.mlp.gate_proj.bias', 'model.layers.28.mlp.down_proj.bias', 'model.layers.0.mlp.down_proj.bias', 'model.layers.9.mlp.gate_proj.bias', 'model.layers.8.self_attn.k_proj.bias', 'model.layers.4.self_attn.v_proj.bias', 'model.layers.1.mlp.up_proj.bias', 'model.layers.11.mlp.down_proj.bias', 'model.layers.5.mlp.up_proj.bias', 'model.layers.20.self_attn.o_proj.bias', 'model.layers.24.self_attn.q_proj.bias', 'model.layers.14.mlp.up_proj.bias', 'model.layers.3.mlp.up_proj.bias', 'model.layers.7.self_attn.q_proj.bias', 'model.layers.1.self_attn.v_proj.bias', 'model.layers.8.mlp.gate_proj.bias', 'model.layers.11.mlp.gate_proj.bias', 'model.layers.12.self_attn.v_proj.bias', 'model.layers.14.self_attn.o_proj.bias', 'model.layers.21.self_attn.o_proj.bias', 'model.layers.3.self_attn.v_proj.bias', 'model.layers.12.self_attn.o_proj.bias', 'model.layers.18.self_attn.k_proj.bias', 'model.layers.1.self_attn.k_proj.bias', 'model.layers.2.self_attn.q_proj.bias', 'model.layers.16.mlp.up_proj.bias', 'model.layers.9.self_attn.v_proj.bias', 'model.layers.6.mlp.gate_proj.bias', 'model.layers.7.mlp.up_proj.bias', 'model.layers.11.self_attn.k_proj.bias', 'model.layers.27.mlp.down_proj.bias', 'model.layers.31.self_attn.v_proj.bias', 'model.layers.5.mlp.down_proj.bias', 'model.layers.5.self_attn.k_proj.bias', 'model.layers.7.self_attn.k_proj.bias', 'model.layers.19.mlp.gate_proj.bias', 'model.layers.22.self_attn.o_proj.bias', 'model.layers.3.self_attn.q_proj.bias', 'model.layers.12.mlp.down_proj.bias', 'model.layers.24.self_attn.o_proj.bias', 'model.layers.3.mlp.gate_proj.bias', 'model.layers.16.self_attn.v_proj.bias', 'model.layers.8.self_attn.v_proj.bias', 'model.layers.22.mlp.gate_proj.bias', 'model.layers.23.self_attn.k_proj.bias', 'model.layers.4.mlp.down_proj.bias', 'model.layers.19.self_attn.k_proj.bias', 'model.layers.9.mlp.up_proj.bias', 'model.layers.30.self_attn.v_proj.bias', 'model.layers.14.self_attn.q_proj.bias', 'model.layers.15.mlp.up_proj.bias', 'model.layers.18.mlp.gate_proj.bias', 'model.layers.4.mlp.up_proj.bias', 'model.layers.0.mlp.gate_proj.bias', 'model.layers.25.mlp.down_proj.bias', 'model.layers.16.mlp.down_proj.bias', 'model.layers.21.self_attn.q_proj.bias', 'model.layers.17.self_attn.q_proj.bias', 'model.layers.18.self_attn.q_proj.bias', 'model.layers.3.mlp.down_proj.bias', 'model.layers.11.self_attn.q_proj.bias', 'model.layers.10.mlp.down_proj.bias', 'model.layers.20.self_attn.v_proj.bias', 'model.layers.20.mlp.gate_proj.bias', 'model.layers.26.self_attn.k_proj.bias', 'model.layers.23.self_attn.v_proj.bias', 'model.layers.25.self_attn.o_proj.bias', 'model.layers.13.mlp.gate_proj.bias', 'model.layers.25.self_attn.k_proj.bias', 'model.layers.14.mlp.down_proj.bias', 'model.layers.9.self_attn.k_proj.bias', 'model.layers.8.self_attn.q_proj.bias', 'model.layers.28.self_attn.k_proj.bias', 'model.layers.24.self_attn.v_proj.bias', 'model.layers.22.self_attn.k_proj.bias', 'model.layers.6.mlp.up_proj.bias', 'model.layers.21.self_attn.k_proj.bias', 'model.layers.7.self_attn.o_proj.bias', 'model.layers.24.mlp.up_proj.bias', 'model.layers.17.self_attn.o_proj.bias', 'model.layers.14.mlp.gate_proj.bias', 'model.layers.20.mlp.up_proj.bias', 'model.layers.26.self_attn.v_proj.bias', 'model.layers.23.mlp.up_proj.bias', 'model.layers.26.mlp.down_proj.bias', 'model.layers.0.self_attn.q_proj.bias', 'model.layers.25.mlp.gate_proj.bias', 'model.layers.23.mlp.gate_proj.bias', 'model.layers.15.self_attn.k_proj.bias', 'model.layers.22.self_attn.v_proj.bias', 'model.layers.7.mlp.down_proj.bias', 'model.layers.2.mlp.down_proj.bias', 'model.layers.2.mlp.gate_proj.bias', 'model.layers.11.mlp.up_proj.bias', 'model.layers.4.self_attn.q_proj.bias', 'model.layers.12.mlp.gate_proj.bias', 'model.layers.7.self_attn.v_proj.bias', 'model.layers.10.self_attn.v_proj.bias', 'model.layers.4.mlp.gate_proj.bias', 'model.layers.2.mlp.up_proj.bias', 'model.layers.27.mlp.up_proj.bias', 'model.layers.29.self_attn.o_proj.bias', 'model.layers.29.self_attn.q_proj.bias', 'model.layers.27.self_attn.v_proj.bias', 'model.layers.5.self_attn.o_proj.bias', 'model.layers.30.mlp.up_proj.bias', 'model.layers.2.self_attn.k_proj.bias', 'model.layers.0.self_attn.o_proj.bias', 'model.layers.12.mlp.up_proj.bias', 'model.layers.24.self_attn.k_proj.bias', 'model.layers.2.self_attn.v_proj.bias', 'model.layers.21.mlp.gate_proj.bias', 'model.layers.8.mlp.down_proj.bias', 'model.layers.19.self_attn.q_proj.bias', 'model.layers.24.mlp.down_proj.bias', 'model.layers.20.self_attn.k_proj.bias', 'model.layers.0.self_attn.k_proj.bias', 'model.layers.15.self_attn.v_proj.bias', 'model.layers.16.self_attn.k_proj.bias', 'model.layers.5.self_attn.v_proj.bias', 'model.layers.18.mlp.up_proj.bias', 'model.layers.13.self_attn.o_proj.bias', 'model.layers.28.mlp.up_proj.bias', 'model.layers.0.mlp.up_proj.bias', 'model.layers.10.self_attn.k_proj.bias', 'model.layers.22.self_attn.q_proj.bias', 'model.layers.31.self_attn.o_proj.bias', 'model.layers.17.self_attn.k_proj.bias', 'model.layers.29.self_attn.k_proj.bias', 'model.layers.23.self_attn.o_proj.bias', 'model.layers.30.mlp.down_proj.bias', 'model.layers.1.self_attn.o_proj.bias', 'model.layers.26.self_attn.q_proj.bias', 'model.layers.4.self_attn.o_proj.bias', 'model.layers.10.mlp.gate_proj.bias', 'model.layers.30.self_attn.q_proj.bias', 'model.layers.13.self_attn.q_proj.bias', 'model.layers.27.mlp.gate_proj.bias', 'model.layers.10.self_attn.o_proj.bias', 'model.layers.15.mlp.down_proj.bias', 'model.layers.31.mlp.down_proj.bias', 'model.layers.6.self_attn.v_proj.bias', 'model.layers.21.mlp.down_proj.bias', 'model.layers.8.self_attn.o_proj.bias', 'model.layers.23.self_attn.q_proj.bias', 'model.layers.27.self_attn.k_proj.bias', 'model.layers.19.mlp.down_proj.bias', 'model.layers.25.self_attn.v_proj.bias', 'model.layers.28.self_attn.q_proj.bias', 'model.layers.29.self_attn.v_proj.bias', 'model.layers.6.self_attn.q_proj.bias', 'model.layers.23.mlp.down_proj.bias', 'model.layers.29.mlp.down_proj.bias', 'model.layers.21.mlp.up_proj.bias', 'model.layers.17.mlp.up_proj.bias', 'model.layers.26.self_attn.o_proj.bias', 'model.layers.31.self_attn.q_proj.bias', 'model.layers.29.mlp.gate_proj.bias', 'model.layers.6.self_attn.o_proj.bias', 'model.layers.30.self_attn.o_proj.bias', 'model.layers.17.mlp.down_proj.bias', 'model.layers.9.mlp.down_proj.bias', 'model.layers.10.self_attn.q_proj.bias', 'model.layers.14.self_attn.k_proj.bias', 'model.layers.18.self_attn.o_proj.bias', 'model.layers.8.mlp.up_proj.bias', 'model.layers.26.mlp.gate_proj.bias', 'model.layers.4.self_attn.k_proj.bias', 'model.layers.30.self_attn.k_proj.bias', 'model.layers.28.self_attn.v_proj.bias', 'model.layers.16.self_attn.o_proj.bias', 'model.layers.9.self_attn.o_proj.bias', 'model.layers.18.self_attn.v_proj.bias', 'model.layers.10.mlp.up_proj.bias', 'model.layers.12.self_attn.q_proj.bias', 'model.layers.1.self_attn.q_proj.bias', 'model.layers.22.mlp.down_proj.bias', 'model.layers.5.self_attn.q_proj.bias', 'model.layers.21.self_attn.v_proj.bias', 'model.layers.3.self_attn.k_proj.bias', 'model.layers.0.self_attn.v_proj.bias'}\n",
            "- This IS expected if you are initializing MistralForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing MistralForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "generation_config.json: 100% 116/116 [00:00<00:00, 593kB/s]\n",
            "Device set to use cuda:0\n",
            "/content/app.py:20: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
            "  llm=HuggingFacePipeline(pipeline=pipe)\n",
            "/content/app.py:33: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory=ConversationBufferWindowMemory(k=3)\n",
            "2025-02-20 21:16:41 - Created default chainlit markdown file at /content/chainlit.md\n",
            "2025-02-20 21:16:41 - Your app is available at http://localhost:8000\n",
            "2025-02-20 21:17:23 - Translation file for pt-BR not found. Using default translation en-US.\n",
            "2025-02-20 21:17:23 - Translated markdown file for pt-BR not found. Defaulting to chainlit.md.\n",
            "/content/app.py:37: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :meth:`~RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html` instead.\n",
            "  llm_chain = ConversationChain(prompt=prompt, llm=llm, memory=memory)\n",
            "2025-02-20 21:17:29 - Translation file for pt-BR not found. Using default translation en-US.\n",
            "/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py:807: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  result = context.run(func, *args)\n",
            "2025-02-20 21:19:18 - Translation file for pt-BR not found. Using default translation en-US.\n",
            "2025-02-20 21:19:52 - Translation file for pt-BR not found. Using default translation en-US.\n",
            "2025-02-20 21:20:59 - Translation file for pt-BR not found. Using default translation en-US.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2025-02-20T21:21:31+0000 lvl=warn msg=\"Stopping forwarder\" name=http-8000-65624825-12a4-4ebb-9580-dfd8eb8d7ba8 acceptErr=\"failed to accept connection: Listener closed\"\n",
            "WARNING:pyngrok.process.ngrok:t=2025-02-20T21:21:31+0000 lvl=warn msg=\"Error restarting forwarder\" name=http-8000-65624825-12a4-4ebb-9580-dfd8eb8d7ba8 err=\"failed to start tunnel: session closed\"\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Aborted!\n"
          ]
        }
      ],
      "source": [
        "# Instalação das dependências\n",
        "!pip3 install chainlit pyngrok langchain optimum auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/ -q\n",
        "\n",
        "# Configuração do ngrok\n",
        "!ngrok config add-authtoken \"2tBX42Q7po6pD4usJ9lkFQ8CuZK_7oXGJvRGWYPeaYKa4s7f7\"\n",
        "\n",
        "# Conectando o ngrok\n",
        "from pyngrok import ngrok\n",
        "print(ngrok.connect(8000).public_url)\n",
        "\n",
        "# Criando o arquivo app.py manualmente\n",
        "app_py_content = \"\"\"\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.prompts.prompt import PromptTemplate\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "import chainlit as cl\n",
        "\n",
        "model_name_or_path = \"TheBloke/zephyr-7B-beta-GPTQ\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name_or_path, device_map=\"auto\")\n",
        "\n",
        "pipe = pipeline(\n",
        "    task=\"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=512\n",
        "    )\n",
        "\n",
        "llm=HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "template = \\\"\"\"\n",
        "You are a helpful assistant that provides information and engages in casual conversation.\n",
        "Respond naturally to user queries and provide useful information.\n",
        "Please, write a single reply only!\n",
        "\n",
        "Current conversation:\n",
        "{history}\n",
        "Question: {input}\n",
        "\\\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(input_variables=[\"history\", \"input\"], template=template)\n",
        "memory=ConversationBufferWindowMemory(k=3)\n",
        "\n",
        "@cl.on_chat_start\n",
        "async def start():\n",
        "    llm_chain = ConversationChain(prompt=prompt, llm=llm, memory=memory)\n",
        "    cl.user_session.set(\"llm_chain\", llm_chain)\n",
        "\n",
        "@cl.on_message\n",
        "async def main(message: cl.message):\n",
        "    llm_chain = cl.user_session.get(\"llm_chain\")\n",
        "    cb = cl.AsyncLangchainCallbackHandler()\n",
        "    cb.answer_reached = True\n",
        "\n",
        "    res = await cl.make_async(llm_chain)(message.content, callbacks=[cb])\n",
        "\n",
        "    response_text = res['response'].split(\"Answer:\")[-1].strip()\n",
        "\n",
        "    await cl.Message(content=response_text).send()\n",
        "\"\"\"\n",
        "\n",
        "# Salvando o conteúdo no arquivo app.py\n",
        "with open(\"app.py\", \"w\") as f:\n",
        "    f.write(app_py_content)\n",
        "\n",
        "# Executando o Chainlit\n",
        "!chainlit run app.py -w"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVDDpvJY2185"
      },
      "source": [
        "#### Faça o cadastro no site ngrok para gerar o seu Token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Td-cn1ou3Fk3"
      },
      "source": [
        "#### <!> Importante <!>\n",
        "#### ::::: SEGURANÇA:::::\n",
        "##### Se disponibilizar em seu repositório este chatbot, para a sua segurança: \"remova o seu token e insira o texto no lugar como SEU_TOKEN_AQUI para evitar problemas futuros ou o uso indevido por terceiros.\"\n",
        "\n",
        "Lembre-se: O seu Token é único e exclusivo para o seu uso e ele é a sua identificação digital!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8MPaO13NBxx"
      },
      "outputs": [],
      "source": [
        "# Atenção: se você quiser rodar mais de uma vez as células, esta aqui só é necessária uma vez no mesmo ambiente de execução\n",
        "!pip3 install chainlit pyngrok langchain optimum auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/ -q\n",
        "\n",
        "\n",
        "!ngrok config add-authtoken \"2tBX42Q7po6pD4usJ9lkFQ8CuZK_7oXGJvRGWYPeaYKa4s7f7\" # Replace your_actual_authtoken with your real authtoken\n",
        "\n",
        "\n",
        "from pyngrok import ngrok\n",
        "print(ngrok.connect(8000).public_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfyVUxBE8dDF"
      },
      "source": [
        "#### Teste do Chatbot final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sr2s6c-L5oHt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3111068d-53a6-44be-a93d-b4292f36d3d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-02-20 21:09:59 - Created default config file at /content/.chainlit/config.toml\n",
            "2025-02-20 21:09:59 - Created default translation directory at /content/.chainlit/translations\n",
            "2025-02-20 21:09:59 - Created default translation file at /content/.chainlit/translations/ja.json\n",
            "2025-02-20 21:09:59 - Created default translation file at /content/.chainlit/translations/en-US.json\n",
            "2025-02-20 21:09:59 - Created default translation file at /content/.chainlit/translations/kn.json\n",
            "2025-02-20 21:09:59 - Created default translation file at /content/.chainlit/translations/gu.json\n",
            "2025-02-20 21:09:59 - Created default translation file at /content/.chainlit/translations/mr.json\n",
            "2025-02-20 21:09:59 - Created default translation file at /content/.chainlit/translations/hi.json\n",
            "2025-02-20 21:09:59 - Created default translation file at /content/.chainlit/translations/ta.json\n",
            "2025-02-20 21:09:59 - Created default translation file at /content/.chainlit/translations/zh-CN.json\n",
            "2025-02-20 21:09:59 - Created default translation file at /content/.chainlit/translations/nl-NL.json\n",
            "2025-02-20 21:09:59 - Created default translation file at /content/.chainlit/translations/bn.json\n",
            "2025-02-20 21:09:59 - Created default translation file at /content/.chainlit/translations/ml.json\n",
            "2025-02-20 21:09:59 - Created default translation file at /content/.chainlit/translations/he-IL.json\n",
            "2025-02-20 21:09:59 - Created default translation file at /content/.chainlit/translations/te.json\n",
            "Usage: chainlit run [OPTIONS] TARGET\n",
            "Try 'chainlit run --help' for help.\n",
            "\n",
            "Error: Invalid value: File does not exist: app.py\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1740085804.617363    2193 init.cc:232] grpc_wait_for_shutdown_with_timeout() timed out.\n"
          ]
        }
      ],
      "source": [
        "!chainlit run app.py -w"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OP3HKAEdTfX1"
      },
      "source": [
        "#### Se der erro, use este comando abaixo para verificar o Chainlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oyViU4wuTqSs"
      },
      "outputs": [],
      "source": [
        "!pip show chainlit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yLn9WD1T03J"
      },
      "source": [
        "#### Se estiver ausente, use este coma do para instalá-lo *** Ele irá reiniciar ou reinicie para que entre em vigor *** Logo, teste o chainlit novamente na célula '!chainlit run app.py -w'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t96gwZ2OUA5a"
      },
      "outputs": [],
      "source": [
        "!pip install chainlit"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}